{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forward-isolation",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "\n",
    "This can only properly be run on a heavy duty PC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-wells",
   "metadata": {},
   "source": [
    "# Imports and folder setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "vulnerable-astronomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing successfully imported.\n"
     ]
    }
   ],
   "source": [
    "from ebay_delivery_prediction_project import preprocessing, Visualisation, preprocessing_models, postprocessing\n",
    "preprocessing.import_test()\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "preprocessed_file_name = \"preprocessed_training_data.csv\"\n",
    "preprocessed_data_folder = \"preprocessed_data\"\n",
    "\n",
    "preprocessed_data_folder_path = os.path.join(\"../data\", preprocessed_data_folder)\n",
    "preprocessed_data_path = os.path.join(preprocessed_data_folder_path, preprocessed_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d89db",
   "metadata": {},
   "source": [
    "# Run this if preprocessing isn't complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "black-stomach",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 15000000 rows.\n",
      "Reading all columns.\n",
      "Index(['b2c_c2c', 'seller_id', 'declared_handling_days',\n",
      "       'acceptance_scan_timestamp', 'shipment_method_id', 'shipping_fee',\n",
      "       'carrier_min_estimate', 'carrier_max_estimate', 'item_zip', 'buyer_zip',\n",
      "       'category_id', 'item_price', 'quantity', 'payment_datetime',\n",
      "       'delivery_date', 'weight', 'weight_units', 'package_size',\n",
      "       'record_number'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# data_object = Preprocessing.read_data(rows_to_read = None, columns = [\"b2c_c2c\"])\n",
    "# data_object = Preprocessing.read_data(rows_to_read = None)\n",
    "\n",
    "training_rows_to_read = 15000000 # This needs to be 15000000\n",
    "\n",
    "training_data = preprocessing.read_data(rows_to_read = training_rows_to_read)[\"train\"]\n",
    "print(training_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-manhattan",
   "metadata": {},
   "source": [
    "# Setting up pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-philip",
   "metadata": {},
   "source": [
    "## Target column\n",
    "\n",
    "The objective is to estimate the total number of calendar days (after payment) it will take to have a purchased item show up at the buyerâ€™s address. This is equivalent to estimating the delivery date using the formula:\n",
    "\n",
    "payment date (local time) + delivery calendar days = delivery date (local time).\n",
    "\n",
    "Participants should provide this delivery date in their submissions.\n",
    "\n",
    "_Now to generate target column_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-rover",
   "metadata": {},
   "source": [
    "### Run the basic preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-electric",
   "metadata": {},
   "source": [
    "#### Before parallelizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "occupational-childhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished parse_datetime_columns\n",
      "Finished create_delivery_calendar_days\n",
      "Finished clean_zip_codes\n"
     ]
    }
   ],
   "source": [
    "training_data = preprocessing.basic_preprocessing(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "likely-store",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.500000e+07\n",
       "mean     4.744132e+00\n",
       "std      3.097165e+00\n",
       "min     -3.540000e+02\n",
       "25%      3.000000e+00\n",
       "50%      4.000000e+00\n",
       "75%      5.000000e+00\n",
       "max      2.140000e+02\n",
       "Name: delivery_calendar_days, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"delivery_calendar_days\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "steady-defensive",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b2c_c2c : 2\n",
      "seller_id : 1759305\n",
      "declared_handling_days : 12\n",
      "acceptance_scan_timestamp : 896695\n",
      "shipment_method_id : 25\n",
      "shipping_fee : 7044\n",
      "carrier_min_estimate : 6\n",
      "carrier_max_estimate : 6\n",
      "item_zip : 50940\n",
      "buyer_zip : 57274\n",
      "category_id : 33\n",
      "item_price : 41571\n",
      "quantity : 147\n",
      "payment_datetime : 12770984\n",
      "delivery_date : 767\n",
      "weight : 1298\n",
      "weight_units : 2\n",
      "package_size : 7\n",
      "record_number : 15000000\n",
      "delivery_calendar_days : 204\n",
      "cleaned_item_zip : 36147\n",
      "cleaned_buyer_zip : 38497\n",
      "distance_between_pincodes : 11915492\n"
     ]
    }
   ],
   "source": [
    "for col in training_data.columns:\n",
    "    print(f\"{col} : {len(training_data[col].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-rebate",
   "metadata": {},
   "source": [
    "# Saving and loading preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "719c0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_file_name = \"preprocessed_training_data.csv\"\n",
    "preprocessed_data_folder = \"preprocessed_data\"\n",
    "\n",
    "preprocessed_data_folder_path = os.path.join(\"../data\", preprocessed_data_folder)\n",
    "preprocessed_data_path = os.path.join(preprocessed_data_folder_path, preprocessed_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af8b2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(preprocessed_data_folder_path):\n",
    "    os.mkdir(preprocessed_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "29ca3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(preprocessed_data_path):\n",
    "    training_data.to_csv(preprocessed_data_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-breathing",
   "metadata": {},
   "source": [
    "# Start running from here if no extra preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "intermediate-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(preprocessed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "commercial-chess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000000, 23)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-determination",
   "metadata": {},
   "source": [
    "# Implementing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-contract",
   "metadata": {},
   "source": [
    "## Arbitrarily choosing how to encode columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "understanding-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "different_types_of_encoding = [\"one-hot\", \"numerical\"]\n",
    "\n",
    "cols_and_encoding = {\n",
    "    \"b2c_c2c\"   : \"one-hot\",\n",
    "    \"seller_id\"   : 6365,\n",
    "    \"declared_handling_days\"   : \"numerical\",\n",
    "    \"acceptance_scan_timestamp\"   : 9848,\n",
    "    \"shipment_method_id\"   : \"one-hot\",\n",
    "    \"shipping_fee\"   : \"numerical\",\n",
    "    \"carrier_min_estimate\"   : \"numerical\",\n",
    "    \"carrier_max_estimate\"   : \"numerical\",\n",
    "    \"item_zip\"   : 4701, # Needs a lot of preprocessing.\n",
    "    \"buyer_zip\"   : 6880, # Needs a lot of preprocessing.\n",
    "    \"category_id\"   : 'one-hot', # Only has 33 unique values in 15,000,000 rows so one-hot.\n",
    "    \"item_price\"   : \"numerical\",\n",
    "    \"quantity\"   : \"numerical\",\n",
    "    \"payment_datetime\"   : 9998,\n",
    "    \"delivery_date\"   : 602,\n",
    "    \"weight\"   : \"numerical\",\n",
    "    \"weight_units\"   : 1,\n",
    "    \"package_size\"   : \"one-hot\",\n",
    "    \"record_number\"   : 10000,\n",
    "    \"delivery_calendar_days\"   : \"target\",\n",
    "    \"cleaned_item_zip\" : 30743,\n",
    "    \"cleaned_buyer_zip\" : 34876,\n",
    "    \"distance_between_pincodes\" : \"numerical\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-courtesy",
   "metadata": {},
   "source": [
    "#### After parallelizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "roman-luxury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols_to_convert_to_one_hot :  ['b2c_c2c', 'shipment_method_id', 'category_id', 'package_size']\n",
      "training_data.shape before :  (1000000, 23)\n",
      "training_data.shape after :  (1000000, 80)\n"
     ]
    }
   ],
   "source": [
    "cols_to_convert_to_one_hot = [ele for ele in cols_and_encoding.keys() if cols_and_encoding[ele] == \"one-hot\"]\n",
    "\n",
    "print(\"cols_to_convert_to_one_hot : \", cols_to_convert_to_one_hot)\n",
    "\n",
    "print(\"training_data.shape before : \", training_data.shape)\n",
    "training_data, generated_columns = preprocessing.one_hot_encode_columns(df = training_data, columns = cols_to_convert_to_one_hot)\n",
    "print(\"training_data.shape after : \", training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eligible-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"delivery_calendar_days\"\n",
    "cols_to_use = [ele for ele in cols_and_encoding.keys() if (type(cols_and_encoding[ele]) == str) and\n",
    "               (cols_and_encoding[ele] != \"target\") and\n",
    "               (cols_and_encoding[ele] != \"one-hot\")]\n",
    "cols_to_use = [*cols_to_use, *generated_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-canadian",
   "metadata": {},
   "source": [
    "## Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "flush-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_prune = { # Percentage of outliers to drop in each column both top and bottom.\n",
    "    \"shipping_fee\" : 0.01,\n",
    "    \"item_price\" : 0.001,\n",
    "    \"distance_between_pincodes\" : 0.001,\n",
    "    \"quantity\" : 0.01,\n",
    "    \"weight\" : 0.01,\n",
    "    \"delivery_calendar_days\" : 0.01, \n",
    "    # This is obviously problematic. Maybe we should have a model that handles outliers in the target\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-college",
   "metadata": {},
   "source": [
    "### Dropping outliers rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "behavioral-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data, columns_to_prune):# This is fundamentally flawed because the thresholds to be pruned should have been calculated earlier not after pruning one or two coloumns.\n",
    "    for col in columns_to_prune.keys():\n",
    "        print(\"col : \", col)\n",
    "        min_and_max = data[col].quantile([columns_to_prune[col], (1-columns_to_prune[col])]).values\n",
    "        all_rows_to_drop = np.append(np.where(data[col] > min_and_max[1])[0], np.where(data[col] < min_and_max[0])[0])\n",
    "#         print(\"all_rows_to_drop : \", all_rows_to_drop)\n",
    "        print(\"all_rows_to_drop.shape : \", all_rows_to_drop.shape)\n",
    "#         return all_rows_to_drop\n",
    "        data.drop(all_rows_to_drop, inplace=True)\n",
    "        print(\"After dropping rows shape is :\", data.shape)\n",
    "        data.reset_index(inplace = True, drop = True)\n",
    "#         data[col] = np.where(data[col] > upper_limit, upper_limit, data[col])\n",
    "#         data[col] = np.where(data[col] < lower_limit, lower_limit, data[col])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unknown-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = remove_outliers(data = training_data, columns_to_prune=columns_to_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-dealing",
   "metadata": {},
   "source": [
    "### Squeezing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "brazilian-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_squeeze = {\n",
    "    \"shipping_fee\",\n",
    "    \"item_price\",\n",
    "    \"distance_between_pincodes\",\n",
    "    \"quantity\",\n",
    "    \"weight\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eight-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_outlier_with_interquantile_range(data, columns):\n",
    "    for col in columns:\n",
    "        sorted(data[col])\n",
    "        Q1, Q3 = data[col].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        lower_limit = Q1 - 1.5 * IQR\n",
    "        upper_limit = Q3 + 1.5 * IQR\n",
    "        data[col] = np.where(data[col] > upper_limit, upper_limit, data[col])\n",
    "        data[col] = np.where(data[col] < lower_limit, lower_limit, data[col])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "radical-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup_training_data = training_data.copy()\n",
    "\n",
    "# training_data = squeeze_outlier_with_interquantile_range(data = training_data, columns=columns_to_squeeze)\n",
    "\n",
    "# backup_training_data[\"shipping_fee\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "imported-lotus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000000.000000\n",
       "mean           2.361428\n",
       "std            3.382954\n",
       "min           -0.250000\n",
       "25%            0.000000\n",
       "50%            0.000000\n",
       "75%            4.000000\n",
       "max           10.000000\n",
       "Name: shipping_fee, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"shipping_fee\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-miniature",
   "metadata": {},
   "source": [
    "### Results and observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-meaning",
   "metadata": {},
   "source": [
    "Choosing a custom percentage to crop off for each column will almost certainlycome in handy.\n",
    "\n",
    "The obvious problem is to detect outliers in delivery_calendar_days.\n",
    "\n",
    "Loss is reduced after we drop outliers from all the column not only delivery_calendar_days.\n",
    "\n",
    "Not sure how to deal with that yet. But one thing to keep in mind is that since the evaluation is not a squared metric, we can be allowed to have a small section of the data be wildly off. If the evaluation was squared then this small section would have had a huge impact on the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-oklahoma",
   "metadata": {},
   "source": [
    "## Running a gradient boosted decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "instrumental-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-fighter",
   "metadata": {},
   "source": [
    "### Custom evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "billion-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLossFunctionEval(y_true, y_pred):\n",
    "    residual = (y_true - y_pred).astype(np.float32)\n",
    "    loss = np.where(residual < 0, 0.4 * (residual), 0.6 * (residual))\n",
    "#     return np.mean(loss)\n",
    "    return \"Custom Loss Function\", np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complete-chase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FDS\n",
      "CPU times: user 105 Âµs, sys: 0 ns, total: 105 Âµs\n",
      "Wall time: 79.9 Âµs\n"
     ]
    }
   ],
   "source": [
    "print(\"FDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "embedded-consensus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((670000, 69), (330000, 69), (670000,), (330000,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To define the input and output feature\n",
    "# x = training_data.drop(['Embarked','PassengerId'],axis=1)\n",
    "x = training_data[cols_to_use]\n",
    "y = training_data[target_column]\n",
    "# train and test split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.33,random_state=42)\n",
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "careful-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swagget/anaconda3/envs/py37/lib/python3.7/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's l2: 8.60008\tvalid_0's Custom Loss Function: 0.16001\n",
      "[20]\tvalid_0's l2: 8.42853\tvalid_0's Custom Loss Function: 0.157311\n",
      "[30]\tvalid_0's l2: 8.38101\tvalid_0's Custom Loss Function: 0.156484\n",
      "[40]\tvalid_0's l2: 8.36465\tvalid_0's Custom Loss Function: 0.156123\n",
      "[50]\tvalid_0's l2: 8.35207\tvalid_0's Custom Loss Function: 0.155838\n",
      "[60]\tvalid_0's l2: 8.34854\tvalid_0's Custom Loss Function: 0.155776\n",
      "[70]\tvalid_0's l2: 8.34368\tvalid_0's Custom Loss Function: 0.155601\n",
      "[80]\tvalid_0's l2: 8.34119\tvalid_0's Custom Loss Function: 0.15557\n",
      "[90]\tvalid_0's l2: 8.33746\tvalid_0's Custom Loss Function: 0.155502\n",
      "[100]\tvalid_0's l2: 8.33811\tvalid_0's Custom Loss Function: 0.155442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=-5, random_state=42)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor(learning_rate=0.1,max_depth=-5,random_state=42)\n",
    "model.fit(x_train,y_train,eval_set=(x_test,y_test),\n",
    "          verbose=10,eval_metric=customLossFunctionEval)# This loss needs to mirror the loss that Ebay is using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-lyric",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_outputs = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(model_test_outputs).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "customLossFunctionEval(y_true = y_test, y_pred = model_test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = {}\n",
    "\n",
    "for importance, feature in zip(model.feature_importances_, model.feature_name_):\n",
    "    feature_importances[feature] = importance\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-graphics",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training accuracy {:.4f}'.format(model.score(x_train,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(model.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data[\"model_outputs\"] = model.predict(training_data[cols_to_use])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "modes_dict = {}\n",
    "for col in training_data:\n",
    "    modes_dict[col] = training_data[col].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-divide",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del training_data\n",
    "quiz_data = preprocessing.read_data(rows_to_read = 10000)[\"quiz\"]\n",
    "\n",
    "quiz_data = preprocessing.parse_datetime_columns(quiz_data)\n",
    "\n",
    "quiz_data, generated_columns = preprocessing.one_hot_encode_columns(df = quiz_data, columns = cols_to_convert_to_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling in the blanks from non existing \n",
    "\n",
    "for col in model.feature_name_:\n",
    "    if col not in quiz_data.columns:\n",
    "        print(\"col : \", col)\n",
    "        quiz_data[col] = modes_dict[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_data[\"model_outputs\"] = model.predict(quiz_data[cols_to_use])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-reducing",
   "metadata": {},
   "source": [
    "# Postprocessing\n",
    "\n",
    "After this is all done and predicting with a decent accuracy this needs to be put back and the output needs to predict the delivery date.\n",
    "\n",
    "The submission is a tsv table, which can be gzipped, with no headeron thw quiz dataset for now. After we win the first phase the test dataset will be given to us.\n",
    "\n",
    "Each line should contain two values, first the record identifier from the distributed dataset, which is an integer, and second the predicted delivery date as a string in the format YYYY-MM-DD.\n",
    "\n",
    "All records must be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_columns_name = \"predicted_delivery_date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-dryer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "postprocessing.generate_output_column(df = quiz_data, \n",
    "                                      predicted_days_column=\"model_outputs\",\n",
    "                                     output_columns_name = output_columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing.generate_submission_file(df = quiz_data, predicted_dates_column=\"predicted_delivery_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-convenience",
   "metadata": {},
   "source": [
    "# Double checking outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"./results/result_2021-12-19 00:30:37.tsv\", sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-mixture",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "747px",
    "left": "58px",
    "top": "111.133px",
    "width": "369.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
